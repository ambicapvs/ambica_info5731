{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ambicapvs/ambica_info5731_spring2021/blob/main/In_class_exercise_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TahL04sVvR"
      },
      "source": [
        "# **The sixth in-class-exercise (20 points in total, 3/2/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyZITr8sjnh"
      },
      "source": [
        "## **1. Rule-based information extraction (10 points)**\n",
        "\n",
        "Use any keywords related to data science, natural language processing, machine learning to search from google scholar, get the **titles** of 100 articles (either by web scraping or manually) about this topic, define a set of patterns to extract the research questions/problems, methods/algorithms/models, datasets, applications, or any other important information about this topic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvR_O9D8sOUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "e2c6e147-69b9-42fe-d9d0-3407d0d44251"
      },
      "source": [
        "!pip install selenium \n",
        "!pip install pandas\n",
        "#!apt-get update \n",
        "#!apt install chromium-chromedriver\n",
        "\n",
        "#Following Code was used to Extract 100 titles for Scholarly articles from Google Scholar for Phrase : DataScience\n",
        "'''\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "import csv\n",
        "import time\n",
        "#chrome_options = webdriver.ChromeOptions()\n",
        "driver = webdriver.Chrome('/Users/prajwalpanchmahalkar/Downloads/chromedriver')\n",
        "#chrome_options.add_argument('--no-sandbox')\n",
        "#chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "#driver =webdriver.Chrome('chromedriver',options=chrome_options) #setup chromedriver options to run in googlecolab \n",
        "\n",
        "with open('scholarlytitles.csv','w') as csv_file:\n",
        "    csvwriter = csv.DictWriter(csv_file, fieldnames=[\"TitleID\", \"Title\"])\n",
        "    csvwriter.writeheader()\n",
        "    i=1\n",
        "    for i in range(0, 100, 10):\n",
        "        driver.get('https://scholar.google.com/scholar?start='+str(i)+'&q=+datascience&hl=en&as_sdt=0,44&as_vis=1')\n",
        "        titles = driver.find_elements_by_class_name(\"gs_rt\")\n",
        "        for title in titles:\n",
        "            csvwriter.writerow({\"TitleID\": str(i), \"Title\": title.text})\n",
        "        time.sleep(2)\n",
        "driver.close()\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom bs4 import BeautifulSoup\\nimport requests\\nfrom selenium import webdriver\\nimport csv\\nimport time\\n#chrome_options = webdriver.ChromeOptions()\\ndriver = webdriver.Chrome(\\'/Users/prajwalpanchmahalkar/Downloads/chromedriver\\')\\n#chrome_options.add_argument(\\'--no-sandbox\\')\\n#chrome_options.add_argument(\\'--disable-dev-shm-usage\\')\\n#driver =webdriver.Chrome(\\'chromedriver\\',options=chrome_options) #setup chromedriver options to run in googlecolab \\n\\nwith open(\\'scholarlytitles.csv\\',\\'w\\') as csv_file:\\n    csvwriter = csv.DictWriter(csv_file, fieldnames=[\"TitleID\", \"Title\"])\\n    csvwriter.writeheader()\\n    i=1\\n    for i in range(0, 100, 10):\\n        driver.get(\\'https://scholar.google.com/scholar?start=\\'+str(i)+\\'&q=+datascience&hl=en&as_sdt=0,44&as_vis=1\\')\\n        titles = driver.find_elements_by_class_name(\"gs_rt\")\\n        for title in titles:\\n            csvwriter.writerow({\"TitleID\": str(i), \"Title\": title.text})\\n        time.sleep(2)\\ndriver.close()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 302
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1acBa3lZC0o",
        "outputId": "96d1f5ea-3cd5-491a-af30-605868006b5c"
      },
      "source": [
        "#Rule-based information extraction\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "'''sentence = \"Trying using Data Science\"\n",
        "sentence2 = \"Using asdasdsad\"\n",
        "sentence3 = \"asdasdsadsadsa Data Science\"\n",
        "'''\n",
        "df = pd.read_csv(\"/content/scholarlytitles.csv\",sep=',',engine ='python', header=0)\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "#sq1 = [{\"LOWER\": \"how\"}]\n",
        "#sq2 = [{\"LOWER\": \"what\"}]\n",
        "sq1 = [{\"TEXT\": {\"REGEX\": \"([^.?!]*)\\\\?\"}}] #Questions\n",
        "sq2 = [{\"LOWER\": \"using\"}, {\"LOWER\":\"data\"}, {\"LOWER\":\"science\"} ] #application/reference\n",
        "sq3 = [{\"LOWER\": \"based\"}, {\"LOWER\":\"on\"}, {\"LOWER\":\"data\"}, {\"LOWER\":\"science\"} ] #application/reference\n",
        "sq4 = [{\"LOWER\": \"with\"}, {\"LOWER\":\"data\"}, {\"LOWER\":\"science\"} ] #application/reference\n",
        "sq5 = [{\"LOWER\": \"in\"}, {\"LOWER\":\"data\"}, {\"LOWER\":\"science\"} ] #application/reference\n",
        "sq6 = [{\"LOWER\": \"of\"}, {\"LOWER\":\"data\"}, {\"LOWER\":\"science\"} ]#application/reference\n",
        "sq7 = [{\"TEXT\": {\"REGEX\":\"problem.*$\"}}] #problems\n",
        "\n",
        "matcher.add(\"ques1\", None, sq1)\n",
        "#matcher.add(\"question2\", None, sq2)\n",
        "#matcher.add(\"question3\", None, sq3)\n",
        "matcher.add(\"use\", None, sq2)\n",
        "matcher.add(\"use2\", None, sq3)\n",
        "matcher.add(\"use3\", None, sq4)\n",
        "matcher.add(\"use4\", None, sq5)\n",
        "matcher.add(\"use5\", None, sq6)\n",
        "matcher.add(\"problem\", None, sq7)\n",
        "for title in df['Title']:\n",
        "  doc = nlp(title)\n",
        "  matches = matcher(doc)\n",
        "  for match_id, start, end in matches:\n",
        "      span = doc[start:end]\n",
        "      print(title)\n",
        "\n",
        "\n"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[BOOK] What is data science?\n",
            "50 years of data science\n",
            "Realizing the potential of data science\n",
            "Big data and data science: what should we teach?\n",
            "An undergraduate degree in data science: curriculum and a decade of implementation experience\n",
            "[BOOK] High-dimensional probability: An introduction with applications in data science\n",
            "Curriculum guidelines for undergraduate programs in data science\n",
            "[PDF] Foundations of data science\n",
            "[BOOK] Principles of data science\n",
            "A first course in data science\n",
            "Data science vs. statistics: two cultures?\n",
            "What is data science? Fundamental concepts and a heuristic example\n",
            "[BOOK] Foundations of data science\n",
            "[BOOK] Data smart: Using data science to transform information into insight\n",
            "Gait biomechanics in the era of data science\n",
            "Inverse statistical problems: from the inverse Ising problem to data science\n",
            "Inverse statistical problems: from the inverse Ising problem to data science\n",
            "Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and …\n",
            "A review of data science in business and industry and a future view\n",
            "[BOOK] Statistical foundations of data science\n",
            "A second chance to get causal inference right: a classification of data science tasks\n",
            "[PDF] 50 years of data science\n",
            "Applications of data science to game learning analytics data: A systematic literature review\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq_7VGmrsum4"
      },
      "source": [
        "## **2. Domain-specific information extraction (10 points)**\n",
        "\n",
        "For the legal case used in the data cleaning exercise: [01-05-1 Adams v Tanner.txt](https://raw.githubusercontent.com/unt-iialab/info5731_spring2021/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt), use [legalNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods) to extract the following inforation from the text (if the information is not exist, just print None):\n",
        "\n",
        "(1) acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
        "\n",
        "(2) amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
        "\n",
        "(3) citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
        "\n",
        "(4) companies, e.g., “Lexpredict LLC”\n",
        "\n",
        "(5) conditions, e.g., “subject to …” or “unless and until …”\n",
        "\n",
        "(6) constraints, e.g., “no more than”\n",
        "\n",
        "(7) copyright, e.g., “(C) Copyright 2000 Acme”\n",
        "\n",
        "(8) courts, e.g., “Supreme Court of New York”\n",
        "\n",
        "(9) CUSIP, e.g., “392690QT3”\n",
        "\n",
        "(10) dates, e.g., “June 1, 2017” or “2018-01-01”\n",
        "\n",
        "(11) definitions, e.g., “Term shall mean …”\n",
        "\n",
        "(12) distances, e.g., “fifteen miles”\n",
        "\n",
        "(13) durations, e.g., “ten years” or “thirty days”\n",
        "\n",
        "(14) geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
        "\n",
        "(15) money and currency usages, e.g., “$5” or “10 Euro”\n",
        "\n",
        "(16) percents and rates, e.g., “10%” or “50 bps”\n",
        "\n",
        "(17) PII, e.g., “212-212-2121” or “999-999-9999”\n",
        "\n",
        "(18) ratios, e.g.,” 3:1” or “four to three”\n",
        "\n",
        "(19) regulations, e.g., “32 CFR 170”\n",
        "\n",
        "(20) trademarks, e.g., “MyApp (TM)”\n",
        "\n",
        "(21) URLs, e.g., “http://acme.com/”\n",
        "\n",
        "(22) addresses, e.g., “1999 Mount Read Blvd, Rochester, NY, USA, 14615”\n",
        "\n",
        "(23) persons, e.g., “John Doe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc7NtJrLx5tS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b683a1-1d55-4b62-e5c6-16d9084b04dc"
      },
      "source": [
        "!pip install pandas\n",
        "!pip install lexnlp\n",
        "!pip install urllib\n",
        "import nltk\n",
        "nltk.download('all')\n"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.15.0)\n",
            "Requirement already satisfied: lexnlp in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: num2words==0.5.10 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.5.10)\n",
            "Requirement already satisfied: datefinder-lexpredict==0.6.2.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.6.2.1)\n",
            "Requirement already satisfied: numpy==1.19.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (1.19.1)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (3.5)\n",
            "Requirement already satisfied: scikit-learn==0.23.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.23.1)\n",
            "Requirement already satisfied: pandas==0.24.2 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.24.2)\n",
            "Requirement already satisfied: requests==2.24.0 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2.24.0)\n",
            "Requirement already satisfied: us==2.0.2 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2.0.2)\n",
            "Requirement already satisfied: scipy==1.5.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (1.5.1)\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (3.8.3)\n",
            "Requirement already satisfied: dateparser==0.7.2 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.7.2)\n",
            "Requirement already satisfied: pycountry==20.7.3 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (20.7.3)\n",
            "Requirement already satisfied: regex==2020.7.14 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2020.7.14)\n",
            "Requirement already satisfied: reporters-db==2.0.3 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2.0.3)\n",
            "Requirement already satisfied: Unidecode==1.1.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (1.1.1)\n",
            "Requirement already satisfied: joblib==0.14.0 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.14.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words==0.5.10->lexnlp) (0.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.7/dist-packages (from datefinder-lexpredict==0.6.2.1->lexnlp) (2.8.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from datefinder-lexpredict==0.6.2.1->lexnlp) (2018.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->lexnlp) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->lexnlp) (4.41.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->lexnlp) (2.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (2020.12.5)\n",
            "Requirement already satisfied: jellyfish==0.6.1 in /usr/local/lib/python3.7/dist-packages (from us==2.0.2->lexnlp) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->lexnlp) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->lexnlp) (4.2.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser==0.7.2->lexnlp) (1.5.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement urllib (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for urllib\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3WN8geSAkeA",
        "outputId": "659e2ee4-5459-4636-e359-b2b0ec2cc067"
      },
      "source": [
        "import pandas as pd\n",
        "import lexnlp\n",
        "import urllib\n",
        "sample_url = 'https://raw.githubusercontent.com/unt-iialab/info5731_spring2021/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt'\n",
        "import requests\n",
        "response = requests.get(sample_url)\n",
        "data = response.text\n",
        "\n",
        "#1. ACTS\n",
        "import lexnlp.extract.en.acts\n",
        "acts = lexnlp.extract.en.acts.get_act_list(data)\n",
        "if acts: \n",
        "  print(acts)\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL5IkbY2Bu7o",
        "outputId": "0c46b694-6a67-479b-b8bd-2c9e45818c85"
      },
      "source": [
        "#2. Amounts\n",
        "import lexnlp.extract.en.amounts\n",
        "amts =lexnlp.extract.en.amounts.get_amounts(data)\n",
        "clean = []\n",
        "for i in amts:\n",
        "  clean.append(str(i))\n",
        "print(clean)"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['5.0', '740.0', '1843.0', '2.0', '1.0', '4.0', '2.0', '1821.0', '5.0', '1.0', '1840.0', '3777.0', '80.0', '100.0', '30.0', '1839.0', '741.0', '22.0', '1840.0', '14000.0', '120.0', '1.0', '1840.0', '3.0', '4.0', '1.0', '1.0', '1840.0', '2.0', '1.0', '361.0', '1.0', '307.0', '6.0', '604.0', '1.0', '2.0', '418.0', '422.0', '7.0', '34.0', '41.0', '167.0', '742.0', '3.0', '112.0', '207.0', '3.0', '338.0', '424.0', '5.0', '26.0', '13.0', '235.0', '8.0', '693.0', '4.0', '1821.0', '167.0', '2.0', '2.0', '216.0', '3.0', '66.0', '4.0', '130.0', '29.0', '2.0', '241.0', '2.0', '332.0', '2.0', '422.0', '9.0', '112.0', '743.0', '9.0', '39.0', '14000.0', '1840.0', '744.0', '5.0', '182.0', '3.0', '368.0', '1.0', '397.0', '6.0', '604.0', '1.0', '1821.0', '167.0', '745.0', '4.0', '746.0', '4.0', '210.0', '46.0', '747.0', '5.0', '5.0', '740.0', '1843.0', '284.0', '2019.0', '9.0', '1.0', '55.0', '266.0', '271.0', '1876.0', '2.0', '47.0', '362.0', '376.0', '1872.0', '3.0', '45.0', '329.0', '334.0', '1871.0', '4.0', '31.0', '526.0', '527.0', '1858.0', '5.0', '21.0', '333.0', '335.0', '1852.0', '6.0', '8.0', '145.0', '147.0', '1857.0', '7.0', '65.0', '256.0', '258.0', '3.0', '1880.0', '8.0', '4.0', '913.0', '914.0', '1887.0', '9.0', '103.0', '464.0', '1936.0', '3.0', '1.0', '9.0', '39.0', '1828.0', '2.0', '2.0', '5.0', '182.0', '1837.0', '2.0', '3.0', '9.0', '108.0', '1812.0', '6.0', '1.0', '2.0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHVqbmxjC3hh",
        "outputId": "7c350851-7a46-4dbb-e2a7-4cec1f1726f0"
      },
      "source": [
        "#3. Citations\n",
        "import lexnlp.extract.en.citations\n",
        "citations = lexnlp.extract.en.citations.get_citations(data)\n",
        "if citations:\n",
        "  print(list(citations))\n",
        "else:\n",
        "  print(\"None\")\n"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(5, 'Ala.', 'Alabama Reports', 740, None, None, None), (5, 'Ala.', 'Alabama Reports', 740, '1843', None, None), (55, 'Ala.', 'Alabama Reports', 266, '271', None, None), (47, 'Ala.', 'Alabama Reports', 362, '376', None, None), (45, 'Ala.', 'Alabama Reports', 329, '334', None, None), (31, 'Ala.', 'Alabama Reports', 526, '527', None, None), (21, 'Ala.', 'Alabama Reports', 333, '335', None, None), (8, 'Cal.', 'California Reports', 145, '147', None, None), (65, 'Ala.', 'Alabama Reports', 256, '258', None, None), (4, 'S.W.', 'South Western Reporter', 913, '914', None, None), (103, 'A.L.R.', 'American Law Reports', 464, None, None, None), (9, 'Cow.', \"Cowen's Reports\", 39, None, None, None), (5, 'Port.', 'Alabama Reports, Porter', 182, None, None, None), (9, 'Johns.', \"Johnson's Reports\", 108, None, None, None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmVXBSU7DTwZ",
        "outputId": "c58648ec-6db1-489d-9cd5-5c61a0e9d1aa"
      },
      "source": [
        "#4. Companies\n",
        "import lexnlp.extract.en.entities.nltk_re\n",
        "companies = lexnlp.extract.en.entities.nltk_re.get_companies(data)\n",
        "if companies:\n",
        "  print(list(companies))\n",
        "else:\n",
        "  print(\"None\")"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Lehman, Durr Co, (18055, 18073)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UKjxL3iDvMZ",
        "outputId": "38f40c53-2b86-4200-b7ef-5512fbb49301"
      },
      "source": [
        "#5. conditions\n",
        "import lexnlp.extract.en.conditions\n",
        "cond = lexnlp.extract.en.conditions.get_conditions(data)\n",
        "if cond:\n",
        "  print(list(cond))\n",
        "else:\n",
        "  print(\"None\")"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('until', '[2]\\r\\nCreditors’ Remedies\\r\\nLien and Priority\\r\\nUnder St.1821, prohibiting a levy on a crop', ''), ('until', 'on a growing crop, nor does such lien attach', ''), ('if', 'It was proved by the claimants, by the production of a written contract, that Harrison, on the twenty-second of May, 1840, in consideration that the claimants were involved, as indorsers for Burton & Harrison of Sumter county, and were then exposed to an execution, amounting to upwards of fourteen thousand dollars, bargained and sold to the claimants all his growing crop of cotton &c., consisting of one hundred and twenty acres, &c. Allen Harrison promised and obliged himself to give up his crop to the use of the claimants at any time to save them from suffering as his indorsers;', ''), ('when', 'The claimants came from Tennessee, (where they resided) about the first of September, 1840, bringing with them three or four white laborers, and took possession of the crop and slaves, and with the latter, and white laborers, gathered the cotton, prepared it for market, and', ''), ('if', 'The court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that Harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but', ''), ('when', 'it was not, and the lien of the fieri facias would have attached upon it,', ''), ('if', 'gathered, yet', ''), ('not subject to', 'the claimants obtained possession on the first of September, and controlled the gathering of the crop, then no lien attached, and it was', ''), ('until', 'Rep, 693;] and', ''), ('until', '167,] which declares it to be lawful to levy an execution on a planted crop,', ''), ('if', 'It is admitted that the contract between the defendant in execution, and the claimants, was in good faith,', ''), ('when', 'The defendant in execution might at any time have divested the interest which the contract vested in the claimants, by discharging their liability as his indorsers, or a judgment creditor might have satisfied the lien, and', ''), ('unless', 'We will then consider the writing under which the claimants assert a right, as a mortgage with a power to take possession any time during the year,', ''), ('if', 'Conceding the truth of the facts stated in the bill of exceptions, and we think it will not follow, that the possession of the claimants is a nullity, and that the case must be considered as', ''), ('if', 'The contract contains an express undertaking to give up the crop at any time the claimants might require it for their indemnity, and', ''), ('if', 'they took possession of it in the absence of the grantor, (though without his consent,)', ''), ('if', 'he subsequently acquiesced in it, the inference would be,', ''), ('subject to', 'Mr. Dane, in remarking upon this point, says, “The American editor of Bacon’s Abridgment, says, ‘Wheat growing in the ground is a chattel, and', ''), ('until', 'The first section of the act of 1821, “To prevent sheriffs and other officers from levying executions in certain cases, enacts, that “It shall not be lawful for any sheriff or other officer, to levy a writ of fieri facias or other execution on the planted crop of a debtor, or person against whom an execution may issue,', ''), ('until', 'Now here is an express inhibition to levy an execution on a crop while it remains on, or in the ground, and', ''), ('until', 'If so, the act cited, will only have the effect of keeping the right to levy it in abeyance', ''), ('if', 'The lien and the right to levy are intimately connected, and', ''), ('until', 'That it was competent for the legislature to have made it unlawful to levy an execution on particular property,', ''), ('until', 'If the object was merely to suspend the sale,', ''), ('as soon as', 'The idea that the lien attached upon the planted crop', ''), ('until', 'the execution was delivered to the sheriff, though the right to levy it was postponed', ''), ('if', 'They do not refer to the lien,', ''), ('until', 'they did they would postpone it', ''), ('until', 'the crop was gathered; but it is the levy they relate to and postpone', ''), ('until', '**4 The right to levy an execution on a planted crop, then, being expressly taken away by the statute, the lien which is connected with and consequent upon that right, never attaches', ''), ('if', 'The circuit judge may have mistaken the law in supposing that the contract was a sale, but', ''), ('when', 'There is no assumption of any material fact in the charge; but the possession of the claimant, the time', ''), ('if', 'acquired, the gathering of the crop, &c., are all referred to the determination of the jury; who are instructed,', ''), ('until', '**4 The statute which presents the question before the court is, that “it shall not be lawful for any sheriff or other officer to levy a writ of fieei facias or other execution, on the planted crop of a debtor, or person against whom an execution may issue,', ''), ('subject to', 'The policy of the State, as indicated by these statutes, is undeniably that all the property of a debtor, real and personal, to which he has a legal title, shall be', ''), ('until', 'The mischief which the statute designed to remedy was, the sacrifice which would be necessarily made by the sale of an immature crop: the statute enables the debtor to retain it', ''), ('if', '**5', ''), ('until', 'The sheriff is forbidden to levy on a “planted crop”', ''), ('if', 'Now,', ''), ('until', 'This, I feel a thorough conviction, was not the intention of the legislature; but that it was to secure him from loss, by prohibiting a levy and sale of the crop,', ''), ('when', 'it was gathered,', ''), ('subject to', 'Growing crops as', ''), ('subject to', '464\\r\\nGenerally, at common law, growing crops raised by annual planting, while still attached to the soil, are regarded as personal chattels,', ''), ('where', 'And', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Estiy8G-ETj1",
        "outputId": "20e18a85-c26b-49b7-c311-fed1767545f2"
      },
      "source": [
        "#6. Constraints \n",
        "import lexnlp.extract.en.constraints\n",
        "cons = lexnlp.extract.en.constraints.get_constraints(data)\n",
        "if cons:\n",
        "  print(list(cons))\n",
        "else:\n",
        "  print(\"None\")"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('after', 'on a growing crop, nor does such lien attach until', ''), ('after', '', ' and that alias and pluries fieri facias’, issued regularly up to the time levy was made; that the cotton levied on was growed on the plantation of harrison, and cultivated by the hands in his service.'), ('first of', 'the claimants came from tennessee, (where they resided) about the', ''), ('first of', 'the court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but if it was not, and the lien of the fieri facias would have attached upon it, when gathered, yet if the claimants obtained possession on the', ''), ('after', 'it merely inhibits the levy, but the lien attaches, and a levy and sale may be made', ''), ('more than', 'taking this to be clear *744 law, and it will be seen, that the defendant in execution at the time of the levy had nothing', ''), ('before', 'it has been frequently mooted whether, at common law, corn, &c.,', ''), ('before', '**4 the statute which presents the question', ''), ('after', 'now, if the view taken by the majority of the court, is correct, the right secured to the plaintiff in execution, of levying on the crop', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S15vOpnfEkv2",
        "outputId": "9f7455a7-26f2-4565-e05d-a7decd4fb661"
      },
      "source": [
        "#7. Copyrights\n",
        "import lexnlp.extract.en.copyright\n",
        "copyrights = lexnlp.extract.en.copyright.get_copyright(data)\n",
        "if copyrights:\n",
        "  print(list(copyrights))\n",
        "else:\n",
        "  print(\"None\")"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('©', '2019', 'Thomson Reuters. No')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQjm4VoxErnR",
        "outputId": "6eddf449-ce3d-4758-9850-0b84e88e9269"
      },
      "source": [
        "#8. Courts\n",
        "#Reference https://lexpredict-lexnlp.readthedocs.io/en/docs-0.1.6/modules/extract_en_courts.html\n",
        "\n",
        "import pandas\n",
        "import lexnlp.extract.en.courts\n",
        "import lexnlp.extract.en.dict_entities\n",
        "from inspect import getmembers, isfunction\n",
        "#print(help(dict_entities))\n",
        "for i,j in getmembers(lexnlp.extract.en.dict_entities, isfunction):\n",
        "  print(i)\n",
        "\n",
        "court_df = pd.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/1.0.5/en/legal/us_courts.csv\")\n",
        "court_config_data = []\n",
        "for _, row in court_df.iterrows():\n",
        "  try:\n",
        "    c = lexnlp.extract.en.dict_entities.entity_config(row[\"Court ID\"], row[\"Court Name\"], 0, row[\"Alias\"].split(\";\") if not pd.isnull(row[\"Alias\"]) else [])\n",
        "    court_config_data.append(c)\n",
        "  except Exception as e:\n",
        "    print(\"The module lexnlp.extract.en.dict_entities is broken there is No entity_config method in https://github.com/LexPredict/lexpredict-lexnlp/blob/f33beecc25b441a2b7a6e2f734d184d5df9d0ae9/lexnlp/extract/en/dict_entities.py\")\n",
        "    break;\n",
        "for entity, alias in lexnlp.extract.en.courts.get_courts(data, court_config_data): #The extraction fails because court_config_data format is unknown as the documentation or code no longer exists on https://github.com/LexPredict/lexpredict-lexnlp\n",
        "    print(\"entity=\", entity)\n",
        "    print(\"alias=\", alias)"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_find_entity_positions\n",
            "alias_is_banlisted\n",
            "conflicts_take_first_by_id\n",
            "conflicts_top_by_priority\n",
            "find_dict_entities\n",
            "get_stem_list\n",
            "get_token_list\n",
            "normalize_text\n",
            "normalize_text_with_map\n",
            "prepare_alias_banlist_dict\n",
            "reverse_src_to_dest_map\n",
            "The module lexnlp.extract.en.dict_entities is broken there is No entity_config method in https://github.com/LexPredict/lexpredict-lexnlp/blob/f33beecc25b441a2b7a6e2f734d184d5df9d0ae9/lexnlp/extract/en/dict_entities.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fVSj7mAHjHv",
        "outputId": "9df3b23e-4dfd-4e13-896f-df123884dbaf"
      },
      "source": [
        "#9. CUSIP\n",
        "import lexnlp.extract.en.cusip\n",
        "cusip = lexnlp.extract.en.cusip.get_cusip_list(data)\n",
        "if cusip:\n",
        "  print(cusip)\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSajN3qjLpqs",
        "outputId": "e46f6018-546a-4724-8fb5-37004104de66"
      },
      "source": [
        "#10. Dates\n",
        "import lexnlp.extract.en.dates\n",
        "dates = lexnlp.extract.en.dates.get_dates(data)\n",
        "datesfmt = []\n",
        "for i in dates:\n",
        "  datesfmt.append(str(i))\n",
        "print(datesfmt)"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2021-06-01', '1840-11-01', '1839-10-01', '1840-09-01', '1840-05-01', '1840-05-01', '2021-12-01', '2021-12-01', '2021-01-01', '2021-01-01', '2021-01-01', '2021-03-21', '2021-06-01', '2021-07-01', '2021-11-01']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtn9yYAQO6IF",
        "outputId": "55a61340-54bf-4558-da7e-8446c6c5ca41"
      },
      "source": [
        "#11. Definitions\n",
        "import lexnlp.extract.en.definitions\n",
        "definitions = lexnlp.extract.en.definitions.get_definitions(data)\n",
        "if list(definitions):\n",
        "  print(list(definitions))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuYH9d3mRajt",
        "outputId": "0e82da6e-b9e6-4d45-e9bf-20a1e0a8e2f2"
      },
      "source": [
        "#12. Distances\n",
        "import lexnlp.extract.en.distances\n",
        "dist = lexnlp.extract.en.distances.get_distances(data)\n",
        "if list(dist):\n",
        "  print(list(definitions))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DojZc4TTSIOY",
        "outputId": "ffeadb92-7030-4a3a-8ca9-ee87728b926c"
      },
      "source": [
        "#13.Durations\n",
        "import lexnlp.extract.en.durations\n",
        "duration = lexnlp.extract.en.durations.get_durations(data)\n",
        "durationfmt = []\n",
        "for i in duration:\n",
        "  durationfmt.append(str(i))\n",
        "print(durationfmt)"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"('second', Decimal('20.0'), Decimal('0.0002'))\", \"('year', Decimal('6.0'), Decimal('2190.0'))\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvqf2I23d_as",
        "outputId": "22eda131-3ac9-423a-96ab-4fddd5bb9f73"
      },
      "source": [
        "#14 Geolocations and Geoentities\n",
        "import lexnlp.extract.en.geoentities\n",
        "import lexnlp.extract.en.dict_entities\n",
        "geo_df = pd.read_csv('https://raw.githubusercontent.com/LexPredict/lexpredict-lexnlp/master/test_data/lexnlp/extract/en/tests/test_geoentities/geoentities.csv')\n",
        "geo_config_data = []\n",
        "for _, row in geo_df.iterrows():\n",
        "  try:\n",
        "    c = lexnlp.extract.en.dict_entities.entity_config(row[\"entity_id\"], row[\"name\"], row[\"category\"])\n",
        "    print(c)\n",
        "    geo_config_data.append(c)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "    print(\"The module lexnlp.extract.en.dict_entities is broken there is No entity_config method in https://github.com/LexPredict/lexpredict-lexnlp/blob/f33beecc25b441a2b7a6e2f734d184d5df9d0ae9/lexnlp/extract/en/dict_entities.py\")\n",
        "    break;\n",
        "for entity, alias in lexnlp.extract.en.geoentities.get_geoentities(data, geo_config_data):\n",
        "    print(\"entity=\", entity)\n",
        "    print(\"alias=\", alias)"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "module 'lexnlp.extract.en.dict_entities' has no attribute 'entity_config'\n",
            "The module lexnlp.extract.en.dict_entities is broken there is No entity_config method in https://github.com/LexPredict/lexpredict-lexnlp/blob/f33beecc25b441a2b7a6e2f734d184d5df9d0ae9/lexnlp/extract/en/dict_entities.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chwJVBeUTHv_",
        "outputId": "3e8188e6-5fd5-4f41-810c-8305a426a435"
      },
      "source": [
        "#15. Currency\n",
        "import lexnlp.extract.en.money\n",
        "money = lexnlp.extract.en.money.get_money(data)\n",
        "if money:\n",
        "  print(list(money))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(Decimal('100.0'), 'USD'), (Decimal('14000.0'), 'USD'), (Decimal('14000.0'), 'USD')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQwdslQATm6Z",
        "outputId": "8526f770-b98d-4c38-b7bc-6f53f187596e"
      },
      "source": [
        "#16. Percentage\n",
        "import lexnlp.extract.en.percents\n",
        "percent = lexnlp.extract.en.percents.get_percents(data)\n",
        "if list(percent):\n",
        "  print(list(percent))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g4iTO5iTyol",
        "outputId": "bd3d6c11-6cd5-4b0a-e364-89fab59921da"
      },
      "source": [
        "#17. Pii Personal Identifiable Information\n",
        "import lexnlp.extract.en.pii\n",
        "pii = lexnlp.extract.en.pii.get_pii(data)\n",
        "if list(pii):\n",
        "  print(list(pii))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5H5K5fAUF8g",
        "outputId": "336ecdbb-32a3-485c-c1b4-5639d0e7d93b"
      },
      "source": [
        "#18. Ratios\n",
        "import lexnlp.extract.en.ratios\n",
        "ratio = lexnlp.extract.en.ratios.get_ratios(data)\n",
        "if list(ratio):\n",
        "  print(list(ratio))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhWIddCbURZf",
        "outputId": "e663906a-69f6-42b5-88bd-dde3fb51d124"
      },
      "source": [
        "#19. Regulations\n",
        "import lexnlp.extract.en.regulations\n",
        "regs = lexnlp.extract.en.regulations.get_regulations(data)\n",
        "if list(regs):\n",
        "  print(list(regs))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TFGlEQbUbpz",
        "outputId": "09866a6a-c40a-4b89-8fde-1dd979a8cf8f"
      },
      "source": [
        "#20. Trademarks\n",
        "import lexnlp.extract.en.trademarks\n",
        "trade = lexnlp.extract.en.trademarks.get_trademarks(data)\n",
        "if list(trade):\n",
        "  print(list(trade))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNz18FUvUhy3",
        "outputId": "dc526677-557b-4088-ceae-c7a17aff9394"
      },
      "source": [
        "#21. URLs\n",
        "import lexnlp.extract.en.urls\n",
        "urls = lexnlp.extract.en.urls.get_urls(data)\n",
        "if list(urls):\n",
        "  print(list(urls))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkGJmWuNU4d8",
        "outputId": "17623e20-68d2-48d6-be8a-bedbf81fc7d6"
      },
      "source": [
        "#22. Addresses\n",
        "import lexnlp.extract.en.addresses.addresses\n",
        "addr = lexnlp.extract.en.addresses.addresses.get_addresses(data)\n",
        "if addr:\n",
        "  print(list(addr))\n",
        "else:\n",
        "  print('None')"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['TANNER AND HORTON. June Term, 1843. Synopsis', '207; 3 Johns. Rep. 338', '2 Johns. Rep. 216; 3', 'Opinion COLLIER, C. J', 'DISSENTING OPINION. ORMOND, J', '2019 Thomson Reuters', 'TROVER FOR CONVERSION OF', 'Hon. A. B. MOORE. Jun', 'Hon. WM. L. WHITLOCK. Nov', '103 A.L.R. 464 Generally', '9 Johns. 108, N.Y.Sup., 1812']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yplPXByrXcNp",
        "outputId": "01ecdead-49d0-4183-ce31-55fedd9a53d1"
      },
      "source": [
        "#23. Person Names\n",
        "import spacy \n",
        "nlp  = spacy.load('en_core_web_sm')\n",
        "sents = nlp(data) \n",
        "namelist = []\n",
        "for name in sents.ents:\n",
        "  if name.label_ == 'PERSON':\n",
        "    namelist.append(str(name).strip())\n",
        "print(namelist)\n"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lien', 'lien attach', 'Allen Harrison', 'Allen Harrison', 'Allen Harrison', 'lien', 'W. G. JONES', 'lien', 'lien', 'lien', 'C. J.', 'Whipple v. Foot', 'Stewart v. Doughty', 'Austin v. Sawyer', 'lien', 'lien', 'Elliott v. Mayfield', 'Dane', 'Whipple v. Foot', 'Poole', 'Whipple v. Foot', 'lien', 'Mansony', 'Hurtell', 'Wood', 'lien', 'J.', 'Clay', 'lien', 'Booker', 'Jones', 'M. J. SAFFOLD', 'JOHN D. CUNNINGHAM', 'Bibb', 'Janney', 'JOHN D. CUNNINGHAM', 'McKenzie', 'Lampley', 'Hon', 'Evans', 'Jun Term', 'Dewey', 'Jacob S. Cohen', 'Cohen', 'L. WHITLOCK', 'Edwards', 'Thompson', 'Austin', 'A. quit-claimed', 'W.', 'Perkins', 'Stewart', 'Doughty']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}